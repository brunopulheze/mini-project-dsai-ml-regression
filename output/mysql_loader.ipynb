{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b207f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV as headerless with names...\n",
      "Raw rows read: 21597\n",
      "Columns: ['id', 'date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'price']\n",
      "Preview:\n",
      "        id     date bedrooms bathrooms sqft_living sqft_lot floors waterfront view condition grade sqft_above sqft_basement yr_built yr_renovated zipcode     lat     long sqft_living15 sqft_lot15  price\n",
      "7129300520 10/13/14        3         1        1180     5650      1          0    0         3     7       1180             0     1955            0   98178 47.5112 -122.257          1340       5650 221900\n",
      "6414100192  12/9/14        3      2.25        2570     7242      2          0    0         3     7       2170           400     1951         1991   98125  47.721 -122.319          1690       7639 538000\n",
      "5631500400  2/25/15        2         1         770    10000      1          0    0         3     6        770             0     1933            0   98028 47.7379 -122.233          2720       8062 180000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_10616\\4270807192.py:53: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_10616\\4270807192.py:58: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df['sale_date'] = pd.to_datetime(df['sale_date'], infer_datetime_format=True, errors='coerce')\n",
      "C:\\Users\\bruno\\AppData\\Local\\Temp\\ipykernel_10616\\4270807192.py:58: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['sale_date'] = pd.to_datetime(df['sale_date'], infer_datetime_format=True, errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned DataFrame shape: (21597, 21)\n",
      "Database `house_price_regression` ensured.\n",
      "Table `house_price_data` ensured (no PRIMARY KEY).\n",
      "Inserting 21597 rows into `house_price_data` in chunks of 5000...\n",
      "  inserted 5000/21597\n",
      "  inserted 10000/21597\n",
      "  inserted 15000/21597\n",
      "  inserted 20000/21597\n",
      "  inserted 21597/21597\n",
      "Insert finished.\n",
      "Done. Rows in `house_price_data` now: 21597\n",
      "You can inspect duplicates with: SELECT id, COUNT(*) c FROM house_price_data GROUP BY id HAVING c>1 ORDER BY c DESC LIMIT 20;\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Import regression_data.csv that has NO header row.\n",
    "- Reads CSV with header=None and assigns correct column names.\n",
    "- Parses the date to YYYY-MM-DD.\n",
    "- Creates database and a table without PRIMARY KEY (preserve duplicates).\n",
    "- Inserts rows in chunks.\n",
    "\n",
    "Usage:\n",
    "  pip install pandas sqlalchemy pymysql\n",
    "  python import_regression_headerless.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import pymysql\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "CSV_PATH = r\"..\\data\\regression_data.csv\"\n",
    "DB_USER = \"root\"\n",
    "DB_PASS = \"123456\"   # <<< REPLACE with your password\n",
    "DB_HOST = \"127.0.0.1\"\n",
    "DB_PORT = 3306\n",
    "DB_NAME = \"house_price_regression\"\n",
    "TARGET_TABLE = \"house_price_data\"   # table without PK to preserve duplicates\n",
    "CHUNKSIZE = 5000\n",
    "# ---------- END CONFIG ----------\n",
    "\n",
    "# Column names in correct order for your CSV (no header in file)\n",
    "COLUMN_NAMES = [\n",
    "    \"id\",\"date\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"floors\",\"waterfront\",\"view\",\n",
    "    \"condition\",\"grade\",\"sqft_above\",\"sqft_basement\",\"yr_built\",\"yr_renovated\",\"zipcode\",\"lat\",\n",
    "    \"long\",\"sqft_living15\",\"sqft_lot15\",\"price\"\n",
    "]\n",
    "\n",
    "def read_csv_headerless(path):\n",
    "    print(\"Reading CSV as headerless with names...\")\n",
    "    # header=None tells pandas the file has no header row; names=... assigns the column names\n",
    "    df = pd.read_csv(path, header=None, names=COLUMN_NAMES, dtype=str, encoding='utf-8', low_memory=False)\n",
    "    print(\"Raw rows read:\", len(df))\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"Preview:\")\n",
    "    print(df.head(3).to_string(index=False))\n",
    "    return df\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # rename date -> sale_date for DB schema consistency\n",
    "    if 'date' in df.columns:\n",
    "        df = df.rename(columns={'date': 'sale_date'})\n",
    "\n",
    "    # Trim whitespace and normalize empty strings -> None\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df = df.replace({'': None, 'NULL': None, 'NaN': None})\n",
    "\n",
    "    # Parse dates (handles mm/dd/yy and mm/dd/YYYY)\n",
    "    if 'sale_date' in df.columns:\n",
    "        df['sale_date'] = pd.to_datetime(df['sale_date'], infer_datetime_format=True, errors='coerce')\n",
    "        df['sale_date'] = df['sale_date'].dt.strftime('%Y-%m-%d')\n",
    "        df.loc[df['sale_date'] == 'NaT', 'sale_date'] = None\n",
    "\n",
    "    # Convert numeric columns to numeric types when reasonable\n",
    "    num_cols = [\n",
    "        \"id\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"floors\",\"waterfront\",\"view\",\"condition\",\"grade\",\n",
    "        \"sqft_above\",\"sqft_basement\",\"yr_built\",\"yr_renovated\",\"lat\",\"long\",\"sqft_living15\",\"sqft_lot15\",\"price\"\n",
    "    ]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            if c == \"id\":\n",
    "                # keep as integer when possible\n",
    "                df[c] = pd.to_numeric(df[c], errors='coerce').astype('Int64')\n",
    "            else:\n",
    "                df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    # Reorder columns to stable order that matches table schema to be created\n",
    "    expected = [\"id\",\"sale_date\",\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\"floors\",\"waterfront\",\"view\",\n",
    "                \"condition\",\"grade\",\"sqft_above\",\"sqft_basement\",\"yr_built\",\"yr_renovated\",\"zipcode\",\"lat\",\n",
    "                \"long\",\"sqft_living15\",\"sqft_lot15\",\"price\"]\n",
    "    available = [c for c in expected if c in df.columns]\n",
    "    df = df[available]\n",
    "    print(\"Cleaned DataFrame shape:\", df.shape)\n",
    "    return df\n",
    "\n",
    "def ensure_database(engine_no_db, db_name):\n",
    "    with engine_no_db.connect() as conn:\n",
    "        conn.execute(text(f\"CREATE DATABASE IF NOT EXISTS `{db_name}` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;\"))\n",
    "    print(f\"Database `{db_name}` ensured.\")\n",
    "\n",
    "def ensure_table(engine_db, table_name):\n",
    "    # Create a table WITHOUT PRIMARY KEY in order to preserve duplicates.\n",
    "    create_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS `{table_name}` (\n",
    "      id BIGINT,\n",
    "      sale_date DATE,\n",
    "      bedrooms TINYINT,\n",
    "      bathrooms DECIMAL(5,2),\n",
    "      sqft_living INT,\n",
    "      sqft_lot INT,\n",
    "      floors DECIMAL(4,2),\n",
    "      waterfront TINYINT,\n",
    "      `view` TINYINT,\n",
    "      `condition` TINYINT,\n",
    "      grade TINYINT,\n",
    "      sqft_above INT,\n",
    "      sqft_basement INT,\n",
    "      yr_built SMALLINT,\n",
    "      yr_renovated SMALLINT,\n",
    "      zipcode VARCHAR(20),\n",
    "      lat DECIMAL(10,6),\n",
    "      `long` DECIMAL(10,6),\n",
    "      sqft_living15 INT,\n",
    "      sqft_lot15 INT,\n",
    "      price BIGINT,\n",
    "      INDEX (zipcode),\n",
    "      INDEX (sale_date),\n",
    "      INDEX (price)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "    \"\"\"\n",
    "    with engine_db.connect() as conn:\n",
    "        conn.execute(text(create_sql))\n",
    "    print(f\"Table `{table_name}` ensured (no PRIMARY KEY).\")\n",
    "\n",
    "def insert_chunks(df, engine_db, table_name, chunksize):\n",
    "    total = len(df)\n",
    "    inserted = 0\n",
    "    print(f\"Inserting {total} rows into `{table_name}` in chunks of {chunksize}...\")\n",
    "    for start in range(0, total, chunksize):\n",
    "        chunk = df.iloc[start:start+chunksize].copy()\n",
    "        # pandas will convert NA to NULL\n",
    "        chunk.to_sql(table_name, con=engine_db, if_exists='append', index=False, chunksize=1000, method='multi')\n",
    "        inserted += len(chunk)\n",
    "        print(f\"  inserted {inserted}/{total}\")\n",
    "    print(\"Insert finished.\")\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        raise SystemExit(f\"CSV file not found: {CSV_PATH}\")\n",
    "\n",
    "    # Step 1: read headerless CSV\n",
    "    df = read_csv_headerless(CSV_PATH)\n",
    "\n",
    "    # Step 2: clean and prepare\n",
    "    df = clean_dataframe(df)\n",
    "\n",
    "    # Step 3: ensure database exists\n",
    "    engine_no_db = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/?charset=utf8mb4\")\n",
    "    ensure_database(engine_no_db, DB_NAME)\n",
    "\n",
    "    # Step 4: engine to database\n",
    "    engine_db = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}?charset=utf8mb4\")\n",
    "\n",
    "    # Step 5: ensure table (no PK)\n",
    "    ensure_table(engine_db, TARGET_TABLE)\n",
    "\n",
    "    # Step 6: insert\n",
    "    insert_chunks(df, engine_db, TARGET_TABLE, CHUNKSIZE)\n",
    "\n",
    "    # Final checks\n",
    "    with engine_db.connect() as conn:\n",
    "        total_rows = conn.execute(text(f\"SELECT COUNT(*) FROM `{TARGET_TABLE}`\")).scalar()\n",
    "    print(f\"Done. Rows in `{TARGET_TABLE}` now: {total_rows}\")\n",
    "    print(f\"You can inspect duplicates with: SELECT id, COUNT(*) c FROM {TARGET_TABLE} GROUP BY id HAVING c>1 ORDER BY c DESC LIMIT 20;\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
